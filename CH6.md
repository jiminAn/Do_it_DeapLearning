# 6강. 2개의 층을 연결합니다 - 다층 신경망

## 06-1. 신경망 알고리즘을 벡터화하여 한 번에 전체 샘플을 사용합니다
: 머신러닝에서는 훈련 데이터를 2차원 배열로 표현하는 경우가 많은데, 2차원 배열은 행을 샘플, 열을 특성으로 생각하면 행렬로 이해할 수 있다. 이번에는 행렬 개념을 신경망 알고리즘에 도입해 보도록 하자.

### 벡터화된 연산을 알고리즘의 성능을 올립니다
: 넘파이, 머신러닝, 딥러닝 패키지들은 다차원 배열의 계산을 빠르게 수행할 수 있다. 즉, 행렬 연산을 빠르게 수행할 수 있다. 이런 기능을 벡터화(vectorization)된 연산이라고 하며, 이 벡터화된 연산하면 알고리즘의 성능을 높일 수 있다. 배치 경사 하강법을 SingleLayer 클래스에 적용하면 벡터화된 연산을 사용할 수 있다.

- 배치 경사 하강법으로 성능을 올립니다. 
: 지금까지 사용한 경사 하강법 알고리즘들(선형 회귀, 로지스틱 회귀) 은 알고리즘을 1번 반복할 때 1개의 샘플을 사용하는 '확률적 경사 하강법'을 사용했다. SingleLayer 클래스에도 확률적 경사 하강법을 사용했다. 이 방법은 가중치를 1번 업데이트 할 때 1개의 샘플을 사용하므로 손실 함수의 전역 최솟값을 불안정하게 찾는다. 하지만 **배치 경사 하강법은 가중치를 1번 업데이트할 때 전체 샘플을 사용하므로 손실 함수의 전역 최솟값을 안정적으로 찾는다.**  단, 배치 경사 하강법은 가중치를 1번 업데이트할 때 사용되는 데이터의 개수가 많으므로 알고리즘 1번 수행당 계산 비용이 많이 든다는 점에 주의해야 한다. 그래서 전체 데이터 세트의 크기가 너무 크면 배치 경사 하강법을 사용하지 못하는 경우도 있다. 지금부터 배치 경사 하강법을 사용하기 위한 벡터화된 연산의 기초를 알아보자

### 벡터 연산과 행렬 연산을 알아봅니다
: 벡터화된 연산을 제대로 사용하기 위해서는 벡터 연산과 행렬 연산을 알아야 한다. 여기서는 신경망에서 자주 사용하는 벡터 연산 중 하나인 점 곱(스칼라 곱)과 행렬 곱셈에 대해 알아보자.

- 점 곱을 알아봅니다.   
: 단일층 신경망에서 z를 구하기 위해 가중치와 입력을 각각 곱하여 더하였다. 그리고 이 계산을 4장의 SingleLayer 클래스 안에 있는 forpass() 메서드에 다음과 같이 구현했다.    
```
z = np.sum(x * self.w ) + self.b  
```
위의 식에서 입력과 가중치의 곱을 간단하게 표현할 수 있는 이유는 넘파이의 원소별 곱셈 기능 덕분이며 다음의 원리로 가중치와 입력의 곱에 대한 합을 한 번에 계산 할 수 있었던 것이다.  
```
x = [x1, x2, ...., xn]  
w = [w1, w2, ...., wn]  
x * w = [x1*w1, x2*w2, ...., xn*wn]
```
이때 x,w는 벡터라고 부르고 벡터는 볼드로 표기한다. 그리고 위의 두 벡터를 곱하여 합을 구하는 계산 (np.sum(x * self.w))을 점 곱(dot product) 또는 스칼라 곱(scalar product)이라고 하는데 앞으로는 두 용어 중 점 곱이라는 말을 사용하겠다. 그리고 수학에서 벡터 a,b 에 대한 점 곱은 a (dot) b와 같이 표기한다. 

- 점 곱을 행렬 곱셈으로 표현합니다. 
: 행 방향으로 놓인 첫 번째 벡터와 열 방향으로 놓인 두 번째 벡터의 원소를 각각 곱한 후 모두 더하는 것과 같다. 행렬 곱셈을 계산하는 넘파이의 np.dot( ) 함수를 사용하면 np.sum ( x * self.w)를 다음과 같이 수정 할 수 있다. 위 행렬의 곱셈 원리를 훈련 데이터의 전체 샘플에 대해 적용하면 배치 경사 하강법을 구현할 수 있다

```
z = np.dot(x,self.w) + self.b
```
- 전체 샘플에 대한 가중치 곱의 합을 행렬 곱셈으로 구합니다. 
: 이제 훈련 데이터의 전체 샘플에 대한 가중치 곱의 합을 행렬 곱셈으로 표현해보겠다. 훈련 데이터의 샘플은 각 샘플이 하나의 행으로 이루어져 있으므로 행렬 곱셈을 적용하면 샘플의 특성과 가중치를 곱하여 더한 행렬을 얻을 수 있다. 일반적으로 행렬 곱셈을 통해 만들어지는 결과 행렬의 크기는 다음과 같이 표기한다
```
(m,n)dot(n,k) = (m,k)
```
이것을 통해 첫 번째 행렬의 열과 두 번째 행렬의 행크기는 반드시 같아야 한다는 점도 알 수 있다. 위의 행렬 곱셈은 앞에서 보았듯이 넘파이의 np.dot( )함수를 사용하면 간단히 계산할 수 있다.
```
np.dot(x, w)
```
이제 행렬 연산을 사용하여  SingleLayer 클래스에 배치 경사 하강법을 적용해보자

###  SingleLayer 클래스에 배치 경사 하강법 적용하기
-> gogo colab!



## 06-2. 2개의 층을 가진 신경망을 구현합니다
### 하나의 층에 여러 개의 뉴런을 사용합니다
### 출력을 하나로 모읍니다
### 은닉층이 추가된 신경망을 알아봅니다
### 다층 신경망의 개념을 정리합니다
### 다층 신경망에 경사 하강법을 적용합니다
### 2개의 층을 가진 신경망 구현하기
### 모델 훈련하기
### 가중치 초기화 개선하기

## 06-3. 미니 배치를 사용하여 모델을 훈련합니다
- 여기서부터


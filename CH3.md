# 3강. 머신러닝의 기초를 다집니다 -수치 예측

## 03-1. 선형 회귀에 대해 알아보고 데이터를 준비합니다
: 머신러닝 알고리즘 중 가장 간단하면서도 딥러닝의 기초가 되는 선형 회귀(Linear Regression)를 만들어보면서 자세히 알아보겠습니다.

### 1차 함수로 이해하는 선형 회귀
: 선형 회귀는 1차 함수로 표현 가능, 1차 함수의 기울기(slope)는 a이고, 절편(intercept)은 b
&ensp;&ensp;&ensp; **y = ax + b** 


- 선형 회귀는 기울기와 절편을 찾아줍니다
: 학교에서 배운 1차 함수의 경우 x에 따른 y값을 찾는데 집중한 반면, 선형 회귀에서는 이와 반대로 x,y가 주어졌을 때 기울기와 절편을 찾는데 집중한다.

- 그래프를 통해 선형 회귀의 문제 해결 과정을 이해해보자
: 그래프에 찍힌 (x,y)좌표로 기울기와 절편을 추정하여 1차 함수를 추정해내는 것 -> **선형 회귀로 만든 모델**  
또한 이런 모델을 통해 새로운 점에 대한 예측을 할 수 있음.  
즉, 미리 준비한 입력(x: 3,4,5)과 타깃(y: 25,32,39)을 가지고 모델(y = 7x + 4)을 만든 다음 새 입력(6)에 대해 어떤 값을 예상한 것, 바로 이 과정이 선형 회귀 모델을 만들어 문제를 해결하는 과정이다

### 문제 해결을 위해 당뇨병 환자의 데이터 준비하기
: 현실적인 문제를 해결해보자. 목표는 '당뇨병 환자의 1년 후 병의 진전된 정도를 예측하는 모델을 만드는 것'. 
문제 해결을 위해 가장 먼저 해야 할 일은 **충분한 양의 입력 데이터**와 **타깃 데이터**를 준비하는 것. 
(참고: ch03. google colab)

  


## 03-2. 경사 하강법으로 학습하는 방법을 알아봅니다

### 그래프로 경사 하강법의 의미를 알아봅니다
 - 선형 회귀와 경사 하강법의 관계를 이해합시다
 : 산점도 그래프를 잘 표현하는 직선의 방정식을 찾는 것이 회귀 알고리즘의 목표였다. 경사 하강법이 바로 그 방법 중 하나이다. 
 **경사 하강법(gradient descent)** 이란, 모델이 데이터를 잘 표현할 수 있도록 기울기(변화율)를 사용하여 모델을 조금씩 조정하는 최적화 알고리즘이다. 이제 경사 하강법을 구현하기 위해 필요한 지식들을 알아보자.
 
 ### 예측값과 변화율에 대해 알아봅니다
 : 딥러닝 분야에서는 기울기 a -> 가중치를 의미하는 ω 혹은 계수를 의미하는 θ 로, y는  -> ŷ (y-hat) 로 표기한다.   
 즉, y = ax + b -> ŷ = ωx + b로 모델을 이해하면 된다.   
 여기서 가중치 ω와 절편 b는 알고리즘이 찾은 규칙을 의미하고, ŷ는 우리가 예측한 값을 의미한다.

- 예측값이란 무엇일까요?
: 입력과 출력 데이터를 통해 규칙을 발견하면 모델을 만들었다고 하는데, 그 모델에 대해 새로운 입력값을 넣으면 어떤 출력 값이 나오게 된다. 
이 값이 모델을 통해 예측한 값, 즉 예측값이다.

### 예측값으로 올바른 모델 찾기
: 앞 식에서 찾아야 할 것은 훈련 데이터(x,y)에 잘 맞는  ω, b이다. ω, b 를 찾기 위한 방법은 아래와 같다
- 훈련 데이터에 잘 맞는  ω, b를 찾는 방법
1. 무작위로  ω, b를 정한다( 무작위로 모델 만들기)
2. x에서 샘플 하나를 선택하여 ŷ을 계산한다(무자위로 모델 예측하기)
3. ŷ과 선택한 샘플의 진짜 y를 비교한다(예측한 값과 진짜 정답 비교하기)
4. ŷ이 y와 더 가까워지도록 ω, b를 조정한다(모델 조정하기)
5. 모든 샘플을 처리할 때까지 다시 2~4항목을 반복한다
(참고: ch03. google colab)

### 변화율로 가중치 업데이트하기
: 지금부터는 x,y에 대한 방정식이 아닌 ω, ŷ에 대한 방정식으로 이해해보자.   
지금부터는  ω와 b를 변화율로 업데이트 하는 방법을 알아볼 것 이다.
(참고: ch03. google colab)

: 이번 실습을 통해 y_hat을 증가시켜야 하는 상황을 가정하고 w,b를 업데이트하는 방법에 대해 알아보았다.  
그러나 이 방법은 다음 두 가지 상황에 대해 적합하게 대처하지 못하므로 수동적인 방법이다.  
- y_hat이 y에 한참 미치치 못하는 값인 경우, w,b를 더 큰 폭으로 수정할 수 없다
- y_hat이 y보다 커지면 y_hat을 감소시키지 못한다

: 이러한 문제를 해결하기 위해 다음 실습에서 w,b를 더 능동적으로 업데이트 하는 방법인 오차 역전을 배워보자

### 오차 역전파로 가중치와 절편을 더 적절하게 업데이트합니다
: 오차 역전파(backpropagation)는 ŷ와 y의 차이를 이용하여 ω와 b를 업데이트합니다. 이 방법은 오차가 연이어 전파되는 모습으로 수행됩니다. 

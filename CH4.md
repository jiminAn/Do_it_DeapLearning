# 4강. 분류하는 뉴런을 만듭니다

## 04-1. 초기 인공지능 알고리즘과 로지스틱 회귀를 알아봅니다.
: 로지스틱 회귀를 제대로 이해하기 위해, 로지스틱 회귀로 발전된 초창기 인공지능 알고리즘들을 순서대로 살펴보자

### 퍼셉트론(Perceptron)에 대해 알아봅니다
: 1957년 코넬 항공 연구소의 프랑크 로젠블라트는 이진 분류 문제에서 최적의 가중치를 학습하는 퍼셉트론 알고리즘을 발표하였다.
여기서 이진 분류(binary classification)란 임의의 샘플 데이터를 True/False로 구분하는 문제를 말하며,  과일이라는 샘플 데이터가 있을 때 사과인지, 아닌지를 판단하는 것이 이진분류에 해당한다

- 퍼셉트론의 전체 구조를 훑어봅니다.
: 퍼셉트론은 직선 방정식을 사용하므로, 3장에서 공부한 선형 회귀와 유사한 구조를 가지고 있다. 하지만 퍼셉트론은 마지막 단계에서 샘플을 이진 분류하기 위하여 계단 함수(step function)라는 것을 사용한다. 계단 함수를 통과한 값을 다시 가중치와 절편을 업데이트(학습)하는데 사용한다.  
뉴런은 입력 신호 w1x1, w2x2, b 를 받아 z를 만든다. 즉 다음 수식에 의해 z를 만드는 것이다. 지금부터 아래의 수식을 '선형 함수'라고 부르겠다. 
**w1x1 + w2x2 + b = z**  
계단 함수는 z가 0보다 크거나 같으면 1로, 0보다 작으면 -1로 분류한다. 이때 1을 양성 클래스(positive class), -1을 음성 클래스(negative class)라고 부르며 위 함수를 그래프로 그리면 계단 모양이 되는 것을 확인할 수 있다.  
쉽게 말해, 퍼셉트론은 선형함수를 통과한 값 z를 계단함수로 보내 0보다 큰지, 작은지 검사하여 1과 -1로 분류하는 아주 간단한 알고리즘이다. 퍼셉트론은 계단 함수의 결과를 사용하여 가중치와 절편을 업데이트한다.

- 지금부터 여러개의 특성을 사용하겠습니다.
: 3장에서 본 뉴런의 특성이 1개인 것에 비해 4장에서 배운 뉴런의 입력 신호 특성에는 2개가 있다.  
앞으로는 여러 특성을 사용하여 문제를 해결하는 경우가 많이 나오므로, 아래와 같이 특성이 n개인 선형 함수 표기법에 익숙해지도록 하자.  
**w1x1 + w2x2 + ... + wnxn + b = z(n번째 특성의 가중치와 입력)**    


### 아달린(Adaline)에 대해 알아봅니다
: 퍼셉트론이 등장한 이후 1960년에 스탠포드 대학의 버나드 위드로우와 테드 호프가 퍼셉트론을 개선한 적응형 선형 뉴런을 발표하였다. 적응현 선형 뉴런은 아달린이라고도 부른다. 아달린은 선형 함수의 결과를 학습에 사용하며, 계단 함수의 결과는 예측에만 활용한다.

### 로지스틱 회귀(logistic regression)에 대해 알아봅니다
: 로지스틱 회귀는 아달린에서 조금 더 발전한 형태를 취하고 있다.
1. 활성화 함수(activation function): 선형 함수를 통과시켜 얻은 z를 임계함수에 보내기 전에 변형시키는 함수
2. 임계 함수(threshold function):  로지스틱 회귀의 마지막 단계에서 사용하는 함수로, 예측을 수행한다.아달린이나 퍼셉트론의  계단 함수와 역할은 비슷하나 활성화 함수의 출력값을 사용한다는 점이 다르다.

- 활성화 함수는 비선형 함수를 사용합니다
: 만약 활성화 함수가 선형 함수일 경우, 활성화 함수(y = kx)와 w1x1 + w2x2 + ... + wnxn + b = z 두 식을 쌓을 경우, 덧셈과 곱셈의 결합법칙과 분배법칙에 의해 정리하면 다시 하나의 큰 선형 함수가 되어 임계 함수 앞에 뉴런을 여러개 쌓아도 결국 선형함수일 것이므로 별 의미가 없게 된다. 그래서 활성화 함수는 의무적으로 비선형 함수를 사용한다. 로지스틱 회귀에는 '시그모이드 함수'가 활성화 함수로 사용되었다

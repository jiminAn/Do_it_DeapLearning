# 4강. 분류하는 뉴런을 만듭니다

## 04-1. 초기 인공지능 알고리즘과 로지스틱 회귀를 알아봅니다.
: 로지스틱 회귀를 제대로 이해하기 위해, 로지스틱 회귀로 발전된 초창기 인공지능 알고리즘들을 순서대로 살펴보자

### 퍼셉트론(Perceptron)에 대해 알아봅니다
: 1957년 코넬 항공 연구소의 프랑크 로젠블라트는 이진 분류 문제에서 최적의 가중치를 학습하는 퍼셉트론 알고리즘을 발표하였다.
여기서 이진 분류(binary classification)란 임의의 샘플 데이터를 True/False로 구분하는 문제를 말하며,  과일이라는 샘플 데이터가 있을 때 사과인지, 아닌지를 판단하는 것이 이진분류에 해당한다

- 퍼셉트론의 전체 구조를 훑어봅니다.  
: 퍼셉트론은 직선 방정식을 사용하므로, 3장에서 공부한 선형 회귀와 유사한 구조를 가지고 있다. 하지만 퍼셉트론은 마지막 단계에서 샘플을 이진 분류하기 위하여 계단 함수(step function)라는 것을 사용한다. 계단 함수를 통과한 값을 다시 가중치와 절편을 업데이트(학습)하는데 사용한다.  
뉴런은 입력 신호 w1x1, w2x2, b 를 받아 z를 만든다. 즉 다음 수식에 의해 z를 만드는 것이다. 지금부터 아래의 수식을 '선형 함수'라고 부르겠다.   
**w1x1 + w2x2 + b = z**  
계단 함수는 z가 0보다 크거나 같으면 1로, 0보다 작으면 -1로 분류한다. 이때 1을 양성 클래스(positive class), -1을 음성 클래스(negative class)라고 부르며 위 함수를 그래프로 그리면 계단 모양이 되는 것을 확인할 수 있다.  
쉽게 말해, 퍼셉트론은 선형함수를 통과한 값 z를 계단함수로 보내 0보다 큰지, 작은지 검사하여 1과 -1로 분류하는 아주 간단한 알고리즘이다. 퍼셉트론은 계단 함수의 결과를 사용하여 가중치와 절편을 업데이트한다.

- 지금부터 여러개의 특성을 사용하겠습니다.    
: 3장에서 본 뉴런의 특성이 1개인 것에 비해 4장에서 배운 뉴런의 입력 신호 특성에는 2개가 있다.  
앞으로는 여러 특성을 사용하여 문제를 해결하는 경우가 많이 나오므로, 아래와 같이 특성이 n개인 선형 함수 표기법에 익숙해지도록 하자.   
**w1x1 + w2x2 + ... + wnxn + b = z(n번째 특성의 가중치와 입력) -> 합이므로 시그마로 표현**    


### 아달린(Adaline)에 대해 알아봅니다
: 퍼셉트론이 등장한 이후 1960년에 스탠포드 대학의 버나드 위드로우와 테드 호프가 퍼셉트론을 개선한 적응형 선형 뉴런을 발표하였다. 적응현 선형 뉴런은 아달린이라고도 부른다. 아달린은 선형 함수의 결과를 학습에 사용하며, 계단 함수의 결과는 예측에만 활용한다.

### 로지스틱 회귀(logistic regression)에 대해 알아봅니다
: 로지스틱 회귀는 아달린에서 조금 더 발전한 형태를 취하고 있다.
1. 활성화 함수(activation function): 선형 함수를 통과시켜 얻은 z를 임계함수에 보내기 전에 변형시키는 함수
2. 임계 함수(threshold function):  로지스틱 회귀의 마지막 단계에서 사용하는 함수로, 예측을 수행한다.아달린이나 퍼셉트론의  계단 함수와 역할은 비슷하나 활성화 함수의 출력값을 사용한다는 점이 다르다.

- 활성화 함수는 비선형 함수를 사용합니다  
: 만약 활성화 함수가 선형 함수일 경우, 활성화 함수(y = kx)와 w1x1 + w2x2 + ... + wnxn + b = z 두 식을 쌓을 경우, 덧셈과 곱셈의 결합법칙과 분배법칙에 의해 정리하면 다시 하나의 큰 선형 함수가 되어 임계 함수 앞에 뉴런을 여러개 쌓아도 결국 선형함수일 것이므로 별 의미가 없게 된다. 그래서 활성화 함수는 의무적으로 비선형 함수를 사용한다. 로지스틱 회귀에는 '시그모이드 함수'가 활성화 함수로 사용되었다


## 04-2. 시그모이드 함수로 확율을 만듭니다

### 시그모이드 함수의 역할을 알아봅니다
: 출력값 z는 활성화 함수를 통과하여 a가 되는데, 이때 로지스틱 회귀에서 사용하는 활성화 함수인 시그모이드 함수는 z를 0~1사이의 확률값으로 변환시켜주는 역할을 합니다. 즉, 시그모이드 함수를 통과한 값 a를 암 종양 판정에 사용하면 양성 샘플일 확율로 해석할 수 있습니다.

### 시그모이드 함수가 만들어지는 과정을 살펴봅니다
: 시그모이드 함수가 만들어지는 과정은 다음과 같습니다.   
 **오즈 비 > 로짓 함수 > 시그모이드 함수**.  
 
 - 오즈 비에 대해 알아볼까요?  
 : 시그모이드 함수는 오즈 비(odds ratio)라는 통계를 기반으로 만들어집니다. 오즈 비는 성공 확률과 실패 확률의 비율을 나타내는 통계이며 다음과 같이 정의 합니다.  
 **OR = p/1-p(p = 성공 확률)**  
 오즈 비를 그래프로 그리면 p가 0~1까지 증가할 때 오즈 비의 값은 처음에는 천천히 증가하지만 1에 가까워지면 급격히 증가합니다.
 
 - 로짓 함수에 대해 알아볼까요?  
 : 오즈 비에 로그 함수를 취하여 만든 함수를 로짓 함수(logit function)라고 합니다. 로짓 함수의 식은 다음과 같습니다.  
 **logit(p) = log(p/1-p)**. 
 로짓 함수는 p가 0.5일 때 0이 되고, p가 0,1일 때 각각 무한대로 음수와 양수가 되는 특징을 가집니다. 
 로짓함수의 세로축을 z, 가로 축을 p로 놓으면 확률이 0~1까지 변할 때, z가 매우 큰 음수에서 매우 큰 양수까지 변하는 것으로 생각할 수 있습니다. 이 식은 다음과 같이 쓸 수 있습니다
 **log(p/1-p) = z**. 
 
 - 로지스틱 함수에 대해 알아볼까요?  
 : 위 식을 다시 z에 대하여 정리하면 다음의 식이 됩니다. z에 대해 정리하는 이유는 가로 축을 z로 놓기 위해서 입니다. 그리고 이 식을 로지스틱 함수라고 부릅니다. 
**p = 1 / 1+e^-z**. 
로지스틱 함수를 그래프로 그려보면 로짓 함수의 가로와 세로 축을 반대로 뒤집어 놓은 모양이 됩니다. 그리고 그래프는 S자 형태를 띄게 됩니다. 이 모양에서 착안하여 로지스틱 함수를 시그모이드 함수(sigmoid function)라고도 부릅니다.

### 로지스틱 회귀 중간 정리하기
: 로지스틱 회귀는 이진 분류가 목표이므로 -무한대~무한대 범위를 가지는 z의 값을 조절할 방법이 필요했습니다. 그래서 시그모이드 함수를 활성화 함수로 사용한 것이죠. 이는 시그모이드 함수를 통과하면 z를 확률처럼 해석할 수 있기 때문입니다. 그리고 시그모이드 함수의 확률인 a를 0,1로 구분하기 위하여 마지막에 임계함수를 사용했습니다. 그 결과 입력 데이터x 는 0 또는 1의 값으로 나누어졌습니다. 즉, 이진 분류가 되었습니다. 즉, 로지스틱 회귀가 이진분류를 하기 위한 알고리즘인 이유를 알았습니다.   
그런데 아직 우리는 가중치와 절편을 적절하게 업데이트 할 수 있는 방법을 배우지 않았습니다. 다음장에서는 로지스틱 회귀를 위한 손실함수인 로지스틱 손실 함수에 대해 알아보겠습니다.


## 04-3. 로지스틱 손실 함수를 경사 하강법에 적용합니다
: 선형 회귀는 정답과 예상값의 오차 제곱이 최소가 되는 가중치와 절편을 찾는 것이 목표인 반면, 로지스틱 회귀와 같은 분류의 목표는 올바르게 분휴된 샘플 데이터의 비율 자체를 높이는 것이다.  
하지만, 안타깝게도 올바르게 분류된 샘플의 비율은 미분 가능한 함수가 아니기 때문에 경사 하강법의 손실 함수로 사용할 수 없다.대신 비슷한 목표를 달성할 수 있는 다른 함수 즉, 로지스틱 손실 함수를 사용하면 된다.

### 로지스틱 손실 함수를 제대로 알아봅시다
: 로지스틱 손실 함수는 다중 분류를 위한 손실 함수인 크로스 엔트로피(cross entropy)손실 함수를 이진 분류 버전으로 만든 것이다. 로지스틱 함수는 다음과 같다. 
**L = -(ylog(a) + (1-y)log(1-a))** (a = 활성화 함수, y = 타깃). 
이진 분류의 경우 그렇다(1), 아니다(0)라는 식으로 2개의 정답만 있으므로 타깃의 값 역시 1 또는 0이다. 따라서 위의 식은 y가 1 이거나 0 인 경우로 정리 됩니다.  
1. y == 1(양성 클래스) : L = -log(a)
2. y == 0(음성 클래스) : L = -log(1-a)  
앞 두식의 값을 최소로 만들다 보면 a의 값이 우리가 원하는 목표치가 됨을 알 수 있는데, 예를 들어 양성 클래스인 경우 로지스틱 손실 함수의 값을 최소로 만들려면 a는 자연스럽게 1에 가까워진다. 이 값을 계단 함수에 통과시키면 올바르게 분류 작업이 수행된다. 즉, 로지스틱 손실 함수를 최소화하면 a의 값이 우리가 가장 이상적으로 생각하는 값이 된다. 이제 로지스틱 손실 함수의 최소값을 만드는 가중치와 절편을 찾기 위해 미분을 해보자

### 로지스틱 손실 함수 미분하기
: 가중치와 절편에 대한 로지스틱 손실 함수의 미분 결과는 다음과 같다. 이 식은 가중치와 절편 업데이트에 사용할 것이다.  
**(∂/∂b)L = -(y-a)1**   
그런데 미분 결과를 확인해보면  ŷ이 a로 바뀌었을 뿐 제곱 오차를 미분한 결과와 동일하다. 즉 로지스틱 회귀의 구현이 3장에서 만든 뉴런 클래스와 크게 다르지 않다는 것을 알 수 있다.    
이 장에서 가장 중요한 것은 **로지스틱 손실 함수의 미분을 통해 로지스틱 손실 함수의 값을 최소로 하는 가중치와 절편을 찾아야 한다는 점**이다

## 04-4. 분류용 데이터 세트를 준비합니다
: 분류 문제를 위해 사이킷런에 포함된 '위스콘신 유방암 데이터 세트'를 데이터 세트를 준비해보자

### 유방암 데이터 세트를 소개합니다.
: 해결할 문제는 유방암 데이터 샘플이 악성 종양인지 혹은 정상 종양인지를 구분하는 이진 분류 문제이다. 
그런데 여기서 주의해야 할 점은 유방암에서 악성이 unnormal, 양성이 normal이나, 이진 분류 문제에서는 해결해야할  목표를 양성 샘플이라고 한다. 즉 해결과제가 악성 종양 판별이므로 양성샘플이 악성 종양인 셈이다. 이 점을 주의하자. 

->  gogo colab!!

## 04-5. 로지스틱 회귀를 위한 뉴런을 만듭니다
:  모델을 만들기 전에 성능 평가에 대해 먼저 잠시 알아보자

### 모델의 성능 평가를 위한 훈련 세트와 테스터 세트
: 훈련된 모델의 실전 성능을 일반화 성능(generalization performance)이라고 부른다. 모델을 학습시킨 훈련 데이터 세트로 다시 모델의 성능을 평가하면 그 모델은 당연히 좋은 성능이 나올 것이다. 이런 성능 평가를 '과도하게 낙관적으로 일반화 성능을 추정한다'고 말한다.    
그렇다면 올바른 모델 성능 측정은 어떻게 이루어져야 할까? 훈련 데이터 세트를 두 덩이로 나누어 하나는 훈련에, 다른 하나는 테스트에 사용하면 된다. 이 때 각각의 덩어리를 훈련 세트, 테스터 세트라고 부른다.  
훈련 세트, 테스터 세트로 나눌 때에는 다음 2가지 규칙을 지켜야 한다.
1. 테스터 세트보다 훈련 세트가 더 많아야 한다
2. 훈련 데이터 세틀을 나누기 전에 양성,음성 클래스가 어느 한 쪽에 몰리지 않도록 골고루 섞어야 한다.

-> gogo colab!!
